{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import emoji\n",
    "import pinyin\n",
    "import math\n",
    "import ciso8601\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions from https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563 that we adapt to get what we want from /r/COVID19positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sub(after, before, sub):\n",
    "\n",
    "    def getPushshiftData(after, before, sub):\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?size=500&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "        # Get up to current time\n",
    "        #url = 'https://api.pushshift.io/reddit/search/submission/?size=500&after='+str(after)+'&subreddit='+str(sub)\n",
    "        print(url)\n",
    "        \n",
    "        # Sometimes get JSONDecdeError so this just sleeps and trys again\n",
    "        while 1:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                data = json.loads(r.text)\n",
    "                return data['data']\n",
    "\n",
    "            except:\n",
    "                print(\"Retrying\")\n",
    "                time.sleep(1)\n",
    "\n",
    "    def collectSubData(subm):\n",
    "        subData = list() #list to store data points\n",
    "        title = subm['title']\n",
    "        url = subm['url']\n",
    "        if 'link_flair_text' in subm:\n",
    "            flair = subm['link_flair_text']\n",
    "        else:\n",
    "            flair = 'NA'\n",
    "        author = subm['author']\n",
    "        sub_id = subm['id']\n",
    "        score = subm['score']\n",
    "        if 'selftext' in subm:\n",
    "            selftext = subm['selftext']\n",
    "        else:\n",
    "            selftext = 'NA'\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "        numComms = subm['num_comments']\n",
    "        permalink = subm['permalink']\n",
    "\n",
    "        subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair,selftext))\n",
    "        subStats[sub_id] = subData\n",
    "        \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    \n",
    "    data = getPushshiftData(after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "\n",
    "    while len(data) > 0:\n",
    "        time.sleep(0.4)\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        # Calls getPushshiftData() with the created date of the last submission\n",
    "        print(len(data))\n",
    "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftData(after, before, sub)\n",
    "        \n",
    "    print(str(len(subStats)) + \" submissions have added to list\")\n",
    "    #print(\"1st entry is:\")\n",
    "    #print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "    #print(\"Last entry is:\")\n",
    "    #print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))\n",
    "    \n",
    "    def updateSubs_file(sub):\n",
    "        upload_count = 0\n",
    "        location = \"data/Scrape/\"\n",
    "        #location = \"data/Scrape/\"\n",
    "        f_name = location + sub + '.csv'\n",
    "        exists_yn = os.path.exists(f_name)\n",
    "        if not(exists_yn):\n",
    "            df2 = pd.DataFrame(\n",
    "                {\"Post ID\": [],\n",
    "                \"Title\": [],\n",
    "                \"Url\": [],\n",
    "                 \"Author\": [],\n",
    "                 \"Score\": [],\n",
    "                 \"Publish Date\": [],\n",
    "                 \"Total No. of Comments\": [],\n",
    "                 \"Permalink\": [],\n",
    "                 \"Flair\": [],\n",
    "                 \"Content\": []\n",
    "                })\n",
    "            df2.to_csv(f_name, sep=',', encoding='utf-8', index=False)\n",
    "        with open(f_name, 'a+', newline='', encoding='utf-8') as file: \n",
    "            a = csv.writer(file, delimiter=',')\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been uploaded\")\n",
    "        \n",
    "    updateSubs_file(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = \"20190101\"\n",
    "start_date = math.floor(time.mktime(ciso8601.parse_datetime(start_date_str).timetuple()))\n",
    "\n",
    "before = math.floor(time.time())\n",
    "subs = pd.read_csv(\"subs\",header=None).values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scrape_info to tell if sub has been scraped and up to where\n",
    "scrape_info_path = \"data/Scrape/Scrape_Info/scrape_info.csv\"\n",
    "\n",
    "if not(os.path.exists(scrape_info_path)):\n",
    "    scrape_info = pd.DataFrame(\n",
    "        {\"Sub\": [],\n",
    "         \"Path\": [],\n",
    "         \"Start\": [],\n",
    "         \"End\": []}\n",
    "    )\n",
    "else:\n",
    "    scrape_info = pd.read_csv(scrape_info_path)\n",
    "    \n",
    "# Add new subs to scrape_info\n",
    "for sub in subs:\n",
    "    if not(any(sub == scrape_info['Sub'])):\n",
    "        scrape_info = scrape_info.append(\n",
    "            pd.DataFrame(\n",
    "                {\"Sub\": [sub],\n",
    "                \"Path\": [\"data/Scrape/\" + sub +  \".csv\"],\n",
    "                #\"Path\": [\"data/Scrape\" + sub + \".csv\"],\n",
    "                \"Start\": [start_date],\n",
    "                \"End\": [start_date]}\n",
    "            ),\n",
    "            ignore_index = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(scrape_info)):\n",
    "    scrape_sub(int(scrape_info['End'][i]), int(before), scrape_info['Sub'][i])\n",
    "    scrape_info.at[i,\"End\"] = before\n",
    "scrape_info.to_csv(scrape_info_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}