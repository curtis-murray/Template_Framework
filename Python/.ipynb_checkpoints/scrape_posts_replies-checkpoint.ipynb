{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import emoji\n",
    "import pinyin\n",
    "import math\n",
    "import ciso8601\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions from https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563 that we adapt to get what we want from /r/Prostate19positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sub(after, before, sub):\n",
    "\n",
    "    def getPushshiftData(after, before, sub):\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?size=500&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "        # Get up to current time\n",
    "        #url = 'https://api.pushshift.io/reddit/search/submission/?size=500&after='+str(after)+'&subreddit='+str(sub)\n",
    "        print(url)\n",
    "        \n",
    "        # Sometimes get JSONDecdeError so this just sleeps and trys again\n",
    "        while 1:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                data = json.loads(r.text)\n",
    "                return data['data']\n",
    "\n",
    "            except:\n",
    "                print(\"Retrying\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "    def getPushshiftDataReplies(parent_link_id):\n",
    "        url = 'https://api.pushshift.io/reddit/search?&limit=10000&link_id='+str(parent_link_id)\n",
    "        # Get up to current time\n",
    "        #url = 'https://api.pushshift.io/reddit/search/submission/?size=500&after='+str(after)+'&subreddit='+str(sub)\n",
    "        #print(url)\n",
    "\n",
    "        # Sometimes get JSONDecdeError so this just sleeps and trys again\n",
    "        while 1:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                data = json.loads(r.text)\n",
    "                return data['data']\n",
    "\n",
    "            except:\n",
    "                print(\"Retrying\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "    def collectComments(parent_link_ids, sub):\n",
    "        \n",
    "            #print(parent_link_ids)\n",
    "\n",
    "            #for parent_link_id in parent_link_ids:\n",
    "        parent_link_id = parent_link_ids\n",
    "        #print(parent_link_id)\n",
    "        data = getPushshiftDataReplies(parent_link_id)\n",
    "        \n",
    "        for subm in data:\n",
    "            #print(subm)\n",
    "            subData = list() #list to store data points\n",
    "            title = 'NA'\n",
    "            url = 'NA'\n",
    "            flair = 'NA'\n",
    "            author = subm['author']\n",
    "            sub_id = subm['id']\n",
    "            score = subm['score']\n",
    "            selftext = subm['body']\n",
    "            created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "            numComms = 'NA'\n",
    "            permalink = subm['permalink']\n",
    "            link_id = subm['link_id']\n",
    "            parent_id = subm['parent_id']\n",
    "\n",
    "            subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair,selftext,link_id,parent_id))\n",
    "            subStats[sub_id] = subData\n",
    "            #print(subData)\n",
    "        return(subData)\n",
    "                    \n",
    "    def collectSubData(subm):\n",
    "        subData = list() #list to store data points\n",
    "        title = subm['title']\n",
    "        url = subm['url']\n",
    "        if 'link_flair_text' in subm:\n",
    "            flair = subm['link_flair_text']\n",
    "        else:\n",
    "            flair = 'NA'\n",
    "        author = subm['author']\n",
    "        sub_id = subm['id']\n",
    "        score = subm['score']\n",
    "        if 'selftext' in subm:\n",
    "            selftext = subm['selftext']\n",
    "        else:\n",
    "            selftext = 'NA'\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "        numComms = subm['num_comments']\n",
    "        permalink = subm['permalink']\n",
    "        link_id = subm['id']\n",
    "        parent_id = 'NA'\n",
    "\n",
    "        subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair,selftext,link_id,parent_id))\n",
    "        \n",
    "        if numComms > 0:\n",
    "            #print(numComms)\n",
    "            subData.append(collectComments(link_id, sub_id))\n",
    "        \n",
    "        subStats[sub_id] = subData\n",
    "\n",
    "\n",
    "        \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    \n",
    "    data = getPushshiftData(after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "\n",
    "    while len(data) > 0:\n",
    "        time.sleep(0.4)\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        # Calls getPushshiftData() with the created date of the last submission\n",
    "        print(len(data))\n",
    "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftData(after, before, sub)\n",
    "        \n",
    "    print(str(len(subStats)) + \" submissions have added to list\")\n",
    "    #print(\"1st entry is:\")\n",
    "    #print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "    #print(\"Last entry is:\")\n",
    "    #print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))\n",
    "    \n",
    "    def updateSubs_file(sub):\n",
    "        upload_count = 0\n",
    "        location = \"data/Prostate/Scrape/\"\n",
    "        #location = \"data/Prostate/Scrape/\"\n",
    "        f_name = location + sub + '.csv'\n",
    "        exists_yn = os.path.exists(f_name)\n",
    "        if not(exists_yn):\n",
    "            df2 = pd.DataFrame(\n",
    "                {\"Post ID\": [],\n",
    "                \"Title\": [],\n",
    "                \"Url\": [],\n",
    "                 \"Author\": [],\n",
    "                 \"Score\": [],\n",
    "                 \"Publish Date\": [],\n",
    "                 \"Total No. of Comments\": [],\n",
    "                 \"Permalink\": [],\n",
    "                 \"Flair\": [],\n",
    "                 \"Content\": [],\n",
    "                 \"link_id\": [],\n",
    "                 \"parent_id\": [],\n",
    "                })\n",
    "            df2.to_csv(f_name, sep=',', encoding='utf-8', index=False)\n",
    "        with open(f_name, 'a+', newline='', encoding='utf-8') as file: \n",
    "            a = csv.writer(file, delimiter=',')\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been uploaded\")\n",
    "        \n",
    "    updateSubs_file(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = \"20190101\"\n",
    "start_date = math.floor(time.mktime(ciso8601.parse_datetime(start_date_str).timetuple()))\n",
    "\n",
    "before = math.floor(time.time())\n",
    "subs = ['ProstateCancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "# Make scrape_info to tell if sub has been scraped and up to where\n",
    "scrape_info_path = \"data/Prostate/Scrape/Scrape_Info/scrape_info.csv\"\n",
    "\n",
    "if not(os.path.exists(scrape_info_path)):\n",
    "    scrape_info = pd.DataFrame(\n",
    "        {\"Sub\": [],\n",
    "         \"Path\": [],\n",
    "         \"Start\": [],\n",
    "         \"End\": []}\n",
    "    )\n",
    "else:\n",
    "    scrape_info = pd.read_csv(scrape_info_path)\n",
    "    \n",
    "# Add new subs to scrape_info\n",
    "for sub in subs:\n",
    "    if not(any(sub == scrape_info['Sub'])):\n",
    "        scrape_info = scrape_info.append(\n",
    "            pd.DataFrame(\n",
    "                {\"Sub\": [sub],\n",
    "                \"Path\": [\"data/Prostate/Scrape/\" + sub +  \".csv\"],\n",
    "                #\"Path\": [\"data/Prostate/Scrape\" + sub + \".csv\"],\n",
    "                \"Start\": [start_date],\n",
    "                \"End\": [start_date]}\n",
    "            ),\n",
    "            ignore_index = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?size=500&after=1606743000&before=1607572071&subreddit=ProstateCancer\n",
      "19\n",
      "2020-12-10 07:26:15\n",
      "https://api.pushshift.io/reddit/search/submission/?size=500&after=1607547375&before=1607572071&subreddit=ProstateCancer\n",
      "20 submissions have added to list\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Prostate/Scrape/ProstateCancer.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-16fce53ebd65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mscrape_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'End'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sub'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mscrape_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"End\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscrape_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_info_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-baa1b40a7dfc>\u001b[0m in \u001b[0;36mscrape_sub\u001b[0;34m(after, before, sub)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" submissions have been uploaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mupdateSubs_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-baa1b40a7dfc>\u001b[0m in \u001b[0;36mupdateSubs_file\u001b[0;34m(sub)\u001b[0m\n\u001b[1;32m    138\u001b[0m                  \u001b[0;34m\"parent_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 })\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Prostate/Scrape/ProstateCancer.csv'"
     ]
    }
   ],
   "source": [
    "for i in range(len(scrape_info)):\n",
    "    scrape_sub(int(scrape_info['End'][i]), int(before), scrape_info['Sub'][i])\n",
    "    scrape_info.at[i,\"End\"] = before\n",
    "scrape_info.to_csv(scrape_info_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
