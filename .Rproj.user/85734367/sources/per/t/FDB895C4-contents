library(lubridate)
library(tidyverse)
library(tidytext)
library(topicmodels)

paths <- list.files(path = "data/Prostate/Scrape/", pattern="*.csv")
#paths <- list.files(path = "data/Prostate/Scrape/", pattern="*.csv")


Posts <- tibble(path = paths) %>% 
	mutate(Sub = str_replace(paths, ".csv", "")) %>% 
	mutate(path = paste("data/Prostate/Scrape/", path, sep = "")) %>% 
	#mutate(path = paste("data/Prostate/Scrape/", path, sep = "")) %>% 
	group_by(Sub) %>% 
	mutate(posts = map("data", ~read_csv(path))) %>% 
	unnest() %>% 
	select(-path) %>% 
	ungroup()

more_stop_words <- c("im", "didnt", "shouldnt", "cant", "wont", "amp",
										 "https", "http", "x200b", "www.reddit.com",
										 "utm_name", "ios_app", "utm_medium")

# Regular expressions to match age
regex_age1 <- "((?i)((age(d*))|(age of )|(m((ale|an))*)|(((I('*|’)(m*))|(he|they|dad|brother|uncle|grandpa|(grand)*father(\\s*('*|’)(i*)))(( am)|( was)|( will be)|( is)|( are)))))\\s*([1-9][0-9]))"
regex_age2 <- "(([1-9][0-9])\\s*((?i)(y(\\s|r|/*\\s*o|(ears* (old)|(young))))|(m((ale|an))*)\\s))"
regex_age3 <- "(\\((?i)m*[1-9][0-9]\\s*(('*’*s*)|(yr*(ears)*(\\s*o*(ld)*))*|(m*))(\\)|,))"

# Merge together to consider all
regex_age <- paste(regex_age1, regex_age2, regex_age3, sep = "|")

# Add ages to prostate data with string that found it (note duplicate posts for duplicate ages)
aged_data <- Posts %>% 
	group_by_all() %>% 
	summarise() %>% 
	ungroup() %>% 
	filter(!is.na(Content)) %>% 
	mutate(Content = str_replace_all(Content, "’", "'")) %>%
	mutate(Content = str_replace_all(Content, "\\.\\.\\.", " ")) %>% 
	filter(!is.na(Content)) %>% 
	mutate(Age = str_extract_all(Content, regex_age)) %>% 
	mutate(ID = 1:n()) %>% 
	unnest(cols = c("Age"), keep_empty = TRUE) %>% 
	mutate(Age_string = Age, 
				 Age = str_extract(Age, "[1-9][0-9]")) %>% 
	mutate(Age = as.numeric(Age))

write_csv(aged_data, path = "data/aged_data.csv")

aged_data_summary <- aged_data %>% 
	mutate(Age_string = str_replace(Age_string, "[1-9][0-9]", "[age]")) %>% 
	group_by(Age_string) %>% 
	summarise(count = n()) %>% 
	arrange(-count) 

aged_data_summary %>%
	write_csv("data/aged_data_string.csv")

hc <- aged_data %>% 
	drop_na() %>% 
	mutate(Age = floor(Age*0.1)*10) %>% 
	group_by(Age) %>% 
	summarise(count = n()) %>% 
	mutate(freq = count/sum(count)) %>% 
	mutate(Age_str = paste(Age, "-", Age+9, sep = "")) %>% 
	mutate(freq_round = round(freq, 3)) %>% 
	hchart(., "column",
				 hcaes(x = Age_str, 
				 			y = freq
				 )
	)%>% 
	hc_yAxis(opposite = FALSE,
					 labels = list(format = "{value}"),
					 title = list(text = "Proportion of posts")) %>% 
	hc_xAxis(opposite = FALSE,
					 labels = list(format = '{value}'),
					 title = list(text = "Age in post")) %>% 
	hc_tooltip(pointFormat = 'Proportion: {point.freq_round} <br/> Occurances: {point.count} <br/>') %>% 
	hc_title(text = "/r/ProstateCancer ages in posts")

path <- "Figures/Plots/Prostate/ages.html"
saveWidget(hc,file.path(normalizePath(dirname(path)),basename(path)))

pre_clean <- aged_data %>% 
	select(Sub,Post_ID = `Post ID`, Title, Author, Date = `Publish Date`, Flair, Content, parent_id, link_id, Score, Age, Age_string) %>% 
	mutate(Post_type = ifelse(is.na(parent_id), "Post", "Response")) %>% 
	map_at(c("link_id", "parent_id"), ~str_remove(.x, "t[1-9]_")) %>% 
	as_tibble() %>% 
	group_by(link_id) %>% 
	nest() %>% 
	mutate(data =
				 	map(data, function(data){
				 		
				 		OP <- data %>% 
				 			filter(Post_type == "Post") %>% 
				 			pull(Author) %>% 
				 			unique() # Aged posts are duplicated if multiple ages appear in the same post
				 		
				 		if(is_empty(OP)){
				 			return()
				 		}
				 	
				 		if(length(OP) > 1){ # Make sure that there are not two original author's, although there won't be.
				 			print(data) 
				 		}
				 		
				 data %>% 
				 	mutate(Original_Author = (Author == OP))
				 	})
	) %>% 
	unnest(data, keep_empty = TRUE)
	
#add_who_variable <- 

people <- tribble(~"Person", ~"word", 
				"Father", c("dad", "father", "pa"),
				"Brother", c("brother"),
				"Uncle", c("uncle"), 
				"Grandfather", c("grandad", "grandfather", "grandpa"),
				"Son", c("son"), 
				"Husband",c("husband", "hubby")) %>% 
	unnest("word")

regex_person <- people %>% 
	mutate(word = paste("(\\b", word, "(('|’)*s)*\\b)", sep = "")) %>% 
	pull(word) %>% 
	paste(collapse = "|")

person_posts <- pre_clean %>%
	mutate(Who = str_extract_all(Content, regex_person)) %>% 
	unnest(Who, keep_empty = TRUE) %>% 
	mutate(Who = str_remove(Who,"('|’)*s\\b")) %>% 
	left_join(people, by = c("Who" = "word")) %>% 
	group_by_at(vars(-Who)) %>% 
	summarise(Person_count = n()) %>% 
	ungroup()

write_csv(person_posts, "data/Prostate/pre_clean.csv")

hc <- person_posts %>% 
	group_by(Post_ID, Person) %>% 
	summarise(value = n()) %>% 
	group_by(Post_ID) %>% 
	mutate(value = value/sum(value)) %>% 
	group_by(Person) %>% 
	summarise(count = sum(value)) %>% 
	ungroup() %>% 
	mutate(freq = count/sum(count)) %>% 
	mutate(freq_round = round(freq*1000)/1000,
				 count_round = round(count)) %>% 
	mutate(Person = ifelse(is.na(Person), "None", Person)) %>% 
	arrange(-count) %>% 
#	ggplot() + 
#	geom_bar(aes(x = Person, y = freq),stat = "identity")
	hchart(., "column",
				 hcaes(x = Person, 
				 			y = freq
				 )
	) %>% 
	hc_yAxis(opposite = FALSE,
					 labels = list(format = "{value}"),
					 title = list(text = "Proportion of posts")) %>% 
	hc_xAxis(opposite = FALSE,
					 labels = list(format = '{value}'),
					 title = list(text = "Person in post")) %>% 
	hc_tooltip(pointFormat = 'Proportion: {point.freq_round} <br/> Occurances: {point.count_round} <br/>') %>% 
	hc_title(text = "/r/ProstateCancer People in posts")

path <- "Figures/Plots/Prostate/people.html"
saveWidget(hc,file.path(normalizePath(dirname(path)),basename(path)))

clean_posts <- person_posts %>% 
	select(-c(Age, Age_string, Person, Person_count)) %>% 
	group_by_all() %>% 
	summarise()
	ungroup() %>% 
	unnest_tokens(word, Content) %>% 
	anti_join(stop_words) %>% 
	filter(!(word %in% more_stop_words)) %>% 
	group_by(word) %>% 
  muatate(count = n()) %>% 
	filter(n() >= 5) %>% 
	ungroup() %>% 
	group_by(Post_ID) %>% 
	filter(n() >= 10) %>%
	#group_by(Sub, Post_ID, Title, Author, Date, Flair) %>% 
	group_by(Sub, Post_ID, Title, Author, Date, Flair, parent_id, link_id) %>% 
	summarise(Content = paste(word, collapse = " ")) %>% 
	ungroup() %>% 
	group_by(Sub, Post_ID, Title, Author, Date, Flair, Content) %>% 
	summarise() %>% 
	ungroup()

write_csv(clean_posts, "data/Prostate/clean_posts.csv")

# Temp ---- just do adelaide for Lewis
# 
# adl_clean_posts <- clean_posts %>% 
# 	filter(Sub == "sydney")
# 
# write_csv(adl_clean_posts, "data/Prostate/adl_clean_posts.csv")

# Sentiment Analysis ------------

# nrc <- get_sentiments("nrc")
# 
# colour_pallete <- read_csv("data/colour_pallete.csv")
# 
# named_colours <- colour_pallete %>% 
# 	spread(sentiment, colour)
# 
# sent_nrc_template <- clean_posts %>% 
# 	group_by(Post_ID) %>% 
# 	select(Post_ID, Date) %>% 
# 	mutate(sentiment = list(c(nrc %>% group_by(sentiment) %>% summarise() %>% pull(sentiment)))) %>% 
# 	unnest() %>% 
# 	mutate(p = 0)
# 
# sent_clean <- clean_posts %>% 
# 	unnest_tokens(word, Content) %>% 
# 	inner_join(nrc, by = "word") %>% 
# 	group_by(Post_ID, Date, sentiment)%>% 
# 	summarise(count = n()) %>% 
# 	group_by(Post_ID, Date) %>% 
# 	mutate(p = count/sum(count)) %>% 
# 	ungroup() %>% 
# 	bind_rows(sent_nrc_template) %>% 
# 	group_by(Post_ID, Date, sentiment) %>% 
# 	summarise(p = sum(p)) %>% 
# 	ungroup()
# 
# my_span <- 0.5
# 
# sent_loess <- sent_clean %>% 
# 	left_join(colour_pallete) %>% 
# 	mutate(colour = str_replace(colour, "#", "")) %>% 
# 	arrange(sentiment) %>% 
# 	group_by(sentiment) %>% 
# 	nest() %>% 
# 	mutate(loess_fitted = map(data, function(data){
# 		data %>% 
# 			mutate(Date = as.numeric(Date)) %>% 
# 			loess(p~Date,., span = my_span) %>% 
# 			fitted()
# 	})) %>% 
# 	unnest()
# 
# p_sent_nrc <- sent_loess %>% 
# 	ggplot() + 
# 	theme_minimal() + 
# 	geom_smooth(aes(Date, p, color = sentiment, linetype = sentiment),
# 							method = "loess", se = F, span = my_span) +
# 	ggrepel::geom_label_repel(data = sent_loess %>%
# 															filter(Date == max(Date)) %>% 
# 															group_by(sentiment, loess_fitted, Date) %>% 
# 															summarise() %>% 
# 															ungroup(),
# 														aes(x = Date, y = loess_fitted, label = sentiment, colour = sentiment), show.legend = F,
# 														nudge_x = 5) +
# 	#geom_line(aes(x = Date, y = loess_fitted), show.legend = F) +
# 	labs(x = "Date", y = "Sentiment proportion") +
# 	#scale_x_continuous(limits = c(1,14), breaks = 1:14, labels = 1:14) +
# 	#theme(legend.position = "none") +
# 	#scale_x_continuous(breaks = 1:14, limits = c(1,16)) +
# 	coord_cartesian(ylim = c(0,NA)) +
# 	#scale_y_continuous(breaks = seq(0,0.25,6)) +
# 	scale_color_manual(values = named_colours)
# 
# p_sent_nrc
# 
# p_sent_nrc_path <- paste("Figures/Plots/Prostate/p_sent_nrc_cities", ".pdf",sep = "")
# 
# ggsave(p_sent_nrc, filename = p_sent_nrc_path,
# 			 width = 10, height = 12)

# LDA ----------

# word_counts <- clean_posts %>%
# 	select(Post_ID, Content) %>%
# 	unnest_tokens(word, Content) %>%
# 	count(Post_ID, word, sort = TRUE)
# 
# #write_csv(word_counts, "word_counts.csv")
# 
# # Express in document term matrix
# po_dtm <- word_counts %>%
# 	cast_dtm(Post_ID, word, n)
# 
# # Perform LDA and find p(word|topic)
# 
# po_lda <- LDA(po_dtm, k = 50, cotrol = list(seed = 2020))
# po_topics <- tidy(po_lda, matrix = "beta")


